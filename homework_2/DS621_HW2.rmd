---
title: "Homework 2"
subtitle: "Classification Metrics"
author: "Group 2"
date: "3/10/2021"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    toc: yes
    toc_levels: yes
---

**Group 2 members:** _Alice Friedman, Diego Correa, Jagdish Chhabria, Orli Khaimova, Richard Zheng, Stephen Haslett_.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load required libraries.
library(tidyverse)
library(caret)
library(pROC)
library(grid)
```


## Assignment Overview

In this homework assignment, you will work through various classification metrics. You will be asked to create functions in R to carry out the various calculations. You will also investigate some functions in packages that will let you obtain the equivalent results. Finally, you will create graphical output that also can be used to evaluate the output of classification models, such as binary logistic regression.

The data set has three key columns we will use:

- **class:** the actual class for the observation.

- **scored.class:** the predicted class for the observation (based on a threshold of 0.5)

- **scored.probability:** the predicted probability of success for the observation


### Task 1

_Download the classification output data set_.

```{r load-data}
data_raw <- read.csv("https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/homework_2/classification-output-data.csv")
```

```{r}
data_raw%>%head(50)
```



### Task 2

_Use the table() function to get the raw confusion matrix for this scored dataset. Make sure you understand the output. In particular, do the rows represent the actual or predicted class? The columns?_

```{r, rawConfusionMatrixTable}
# For the confusion matrix, we are only interested in the class and scored.class variables,
# so we select only these variables and ignore the rest.
confusion_matrix_table <- data_raw %>%
  select(class, scored.class)

# For readability purposes, rename 'scored.class' to Predicted, and 'class' to Actual.
dplyr::rename(confusion_matrix_table, Predicted = scored.class, Actual = class) %>%
    # Convert numeric boolean values to human readable values.
    mutate(Predicted = recode(Predicted,
                               '0' = 'Negative',
                               '1' = 'Positive'),
         Actual = recode(Actual,
                        '0' = 'Negative',
                        '1' = 'Positive')) %>%
    table()
```


## Functions

```{r, selectDataForFunctions}
# We only need the class and scored.class variables from the dataset so we
# extract them and leave everything else.
data <- data_raw %>%
  select(class, scored.class)
```

### Task 3: Accuracy Function

_Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the_ _predictions_.

$$Accuracy = \frac{TP + TN}{TP + FP + TN + FN}$$

```{r, accuracyFunction}
accuracy <- function(df,col1,col2) {
  true = df[,col1]
  predict = df[,col2]
  # total events
  len = length(true)
  # total correct predictions
  correct = 0
  for (i in seq(len)){
    if (true[i] == predict[i]){
      correct = correct + 1
    }
  }
  # accuracy
  return (correct/len)
}
#example 
accuracy(data_raw,'class','scored.class')

```


### Task 4: Classification Error Rate Function

_Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions_.

$$Classification Error Rate = \frac{FP + FN}{TP + FP + TN + FN}$$

```{r, classificationErrorRateFunction}

class_error_rate <- function(df,col1,col2) {
  true = df[,col1]
  predict = df[,col2]
  # total events
  len = length(true)
  # total errors
  error = 0
  for (i in seq(len)){
    if (true[i] != predict[i]){
      error = error + 1
    }
  }
  # error rate
  return (error/len)
}
#example 
class_error_rate(data_raw,'class','scored.class')


```


### Task 5: Precision Function

_Write a function that takes the data set as a dataframe, with actual and predicted classifications identified_,
_and returns the precision of the predictions_.

$$Precision = \frac{TP}{TP + FP}$$


```{r, predictionPrecisionFunction}
#' Precision
#'
#' Given a dataset of actual and predicted classifications,
#' returns the precision of the predictions.
#'
#' @param data A dataset of actual and predicted classifications.
#'
#' @return Precision of predictions as a numeric value rounded to 2 decimal places.
precision <- function(data) {
  # Calculate the total number of true positives in the dataset.
  true_positive <- sum(data$class == 1 & data$scored.class == 1)

  # Calculate the total number of false positives in the dataset.
  false_positive <- sum(data$class == 0 & data$scored.class == 1)

  # Perform the precision calculation and round the result to 2 decimal places.
  prediction_precision <- round(true_positive / (true_positive + false_positive), 2)

  return(prediction_precision)
}

# Call the function to provide example output.
precision(data)
```


### Task 6: Sensitivity Function

_Write a function that takes the data set as a dataframe, with actual and predicted classifications identified_,
_and returns the sensitivity of the predictions. Sensitivity is also known as recall_.

$$Sensitivity = \frac{TP}{TP + FN}$$

```{r, sensitivityFunction}
#note: there is a built in function in package caret called sensitivity
# head(data)

sensitivity <- function(data) {
  
  true_positive <- sum(data$class == 1 & data$scored.class == 1)
  false_negative <- sum(data$class == 1 & data$scored.class == 0)
  
  sensitivity <- true_positive / (true_positive + false_negative)
   
  return(sensitivity)
} 

sensitivity(data)
```


### Task 7: Specificity Function

_Write a function that takes the data set as a dataframe, with actual and predicted classifications identified_,
_and returns the specificity of the predictions_.

$$Specificity = \frac{TN}{TN + FP}$$

```{r, specificityFunction}

specificity <- function(data) {
  
  true_negative <- sum(data$scored.class == 0 & data$class == 0)
  false_positive <- sum(data$scored.class == 1 & data$class == 0)
  
  specificity <- true_negative / (true_negative + false_positive) 
  
  return(specificity)
}

specificity(data)
```


### Task 8: F1 Score Function

_Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,_
_and returns the F1 score of the predictions._

$$F1 Score = \frac{2 \times Precision \times Sensitivity }{Precision + Sensitvity}$$

```{r, f1ScoreFunction}
# Should be based on the previous functions, so something like the below...
f1_score <- function(data) {
  sens <- sensitivity(data)
  prec <- precision(data)
  f1 <- 2 * sens * prec / (prec+sens)
  return(f1)
}

f1_score(data)

```


### Task 9

_What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1_.

```{r, F1Bounds}
#data1<-data.frame(rep(0,5),rep(0,5))
#data1<-data.frame(rep(0,5),rep(1,5))
#data1<-data.frame(rep(1,5),rep(0,5))
data1<-data.frame(rep(1,5),rep(1,5))
colnames(data1)<-c('class','scored.class')
data1
precision(data1)
sensitivity(data1)
f1_score(data1)
```




### Task 10

_Write a function that generates an ROC curve from a data set with a true classification column (class in our_
_example) and a probability column (scored.probability in our example). Your function should return a list_
_that includes the plot of the ROC curve and a vector that contains the calculated area under the curve_
_(AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals_.

```{r, ROCFunction, message=FALSE, warning=FALSE, error=FALSE}
roc_curve_generator <- function(class, probability) {
  # Loop through the thresholds and create a dataframe for each item (0 through 1 in increments of 0.01).
  for (threshold in seq(0, 1, 0.01)) {
    threshold_data <- data.frame(class = class,
                scored.class = if_else(probability >= threshold, 1, 0),
                scored.probability = probability)

    # Utilize the custom specificity() and sensitivity() functions we defined earlier
    # to calcualate the True positive Rate (TPR), and False Positive Rate (FPR).
    if (!exists('FPR') & !exists('TPR')) {
      FPR <- 1 - specificity(threshold_data)
      TPR <- sensitivity(threshold_data)
    }
    else {
      FPR <- c(FPR, 1 - specificity(threshold_data))
      TPR <- c(TPR, sensitivity(threshold_data))
    }
  }

  # Calculate the Area Under the Curve (AUC) rounded to 2 decimal places.
  roc_data <- data.frame(TPR, FPR) %>% arrange(FPR)
  AUC <- round(sum(roc_data$TPR * c(diff(roc_data$FPR), 0)) + sum(c(diff(roc_data$TPR)) * c(diff(roc_data$FPR), 0)) / 2, 2)

  # Generate the ROC Curve plot.
  plot(FPR, TPR, 'l', main = 'ROC Curve Plot', xlab = 'Specificity', ylab = 'Sensitivity')
  grid (10, 10, lty = 1, col = 'palevioletred1')
  polygon(c(FPR, 1, 1), c(TPR, 0, 1), col = 'green', density = 55, angle = 50)
  polygon(c(0, 0, 1, 1), c(0, 1, 1, 0), col = 'black', density = 0, lty = 6)
  abline(a = 0, b = 1, col = 'red', lwd = 1.8)
  legend(0.5, 0.45, AUC, title = 'AUC', cex = 1.2)
}

# Call the function to generate example output.
roc_curve_generator(data_raw$class, data_raw$scored.probability)
```


### Task 11

_Use your **created R functions** and the provided classification output data set to produce all of the_
_classification metrics discussed above_.

```{r}
acc = accuracy(data_raw,'class','scored.class')
class_error = class_error_rate(data_raw,'class','scored.class')
prec = precision(data)
sens = sensitivity(data)
spec = specificity(data)
f1 = f1_score(data)
paste("Accuracy is: ",acc)
paste("Class Error Rate is: ",class_error)
paste("Precision is: ", prec)
paste("Sensitivity is: ", sens)
paste("Specificity is: ",spec)
paste("F1 Score is: ",f1)
```

### Task 12

_Investigate the **caret** package. In particular, consider the functions confusionMatrix, sensitivity, and_
_specificity. Apply the functions to the data set. How do the results compare with your own functions_?

```{r}
#convert the variables into factors as needed for the confusionMatrix
data <- data %>%
  mutate(scored.class = as.factor(scored.class),
         class = as.factor(class))

confusionMatrix(data$scored.class, data$class, positive = "1")

caret::sensitivity(data$scored.class, data$class, positive = "1")

caret::specificity(data$scored.class, data$class, negative = "0")
```

### Task 13

_Investigate the **pROC** package. Use it to generate an ROC curve for the data set. How do the results_
_compare with your own functions_?

```{r}
pROC <- roc(data_raw$class, data_raw$scored.probability)
plot(pROC, main = "pROC curve")
```
